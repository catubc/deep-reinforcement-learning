{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e130125-decf-4e76-9786-7cf878370fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib.use('Agg')\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict, deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib tk\n",
    "\n",
    "#\n",
    "from agent import Agent\n",
    "from monitor import interact\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f5da43-a19e-4b6f-940a-c9dbdd6a3809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### Actions\n",
    "# There are 6 discrete deterministic actions:\n",
    "# - 0: move south\n",
    "# - 1: move north\n",
    "# - 2: move east\n",
    "# - 3: move west\n",
    "# - 4: pickup passenger\n",
    "# - 5: drop off passenger\n",
    "\n",
    "# Passenger locations/Destinations:\n",
    "# - 0: R(ed)\n",
    "# - 1: G(reen)\n",
    "# - 2: Y(ellow)\n",
    "# - 3: B(lue)\n",
    "# - 4: in taxi\n",
    "\n",
    "#\n",
    "# hierarchy of states;\n",
    "# num_rows = 5\n",
    "# num_columns = 5\n",
    "# locs = env.locs\n",
    "# num_actions = 6\n",
    "# for row in range(num_rows):\n",
    "#     for col in range(num_columns):\n",
    "#         for pass_idx in range(len(locs) + 1):  # +1 for being inside taxi\n",
    "#             for dest_idx in range(len(locs)):\n",
    "#                 state = env.encode(row, col, pass_idx, dest_idx)\n",
    "#                 print (\"pass idx: \", pass_idx, \", dest: \", dest_idx, \", state: \", state)\n",
    "#                 for action in range(num_actions):\n",
    "#                     pass\n",
    "                \n",
    "#                     #self.P[state][action].append(\n",
    "#                     #            (1.0, new_state, reward, terminated)\n",
    "#                     #        )\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45e65a2c-1df0-4edf-abf8-f51e7916dbe3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 1000000/1000000 [2:50:45<00:00, 97.60it/s, average_reward=-142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "################## TRAIN AGENT #######################\n",
    "######################################################\n",
    "\n",
    "#\n",
    "env = gym.make('Taxi-v3')\n",
    "nA = 6\n",
    "epsilon = 0.25\n",
    "epsilon_scaling = 1.000015\n",
    "gamma = 1.0\n",
    "alpha = 0.2\n",
    "agent = Agent(nA,\n",
    "              epsilon,\n",
    "              epsilon_scaling,\n",
    "              alpha,\n",
    "              gamma)\n",
    "\n",
    "#\n",
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "verbose = False\n",
    "\n",
    "# trianing state\n",
    "agent.greedy_policy=False\n",
    "\n",
    "#\n",
    "current_reward = 0\n",
    "last_reward = 0\n",
    "i_episodes = np.arange(1000000)\n",
    "\n",
    "#\n",
    "with tqdm(i_episodes) as pbar:\n",
    "    \n",
    "    # loop over episodes\n",
    "    for i_episode in pbar:\n",
    "        \n",
    "        #\n",
    "        env.reset()\n",
    "        \n",
    "        #\n",
    "        #env.s = 0\n",
    "\n",
    "        #\n",
    "        agent.i_episode = i_episode\n",
    "\n",
    "        # figrue out where agent and target is inthe world\n",
    "        taxi_row, taxi_col, pass_idx, dest_idx = env.decode(env.s)\n",
    "\n",
    "   \n",
    "        # loop until we pickup the passanger\n",
    "        state_t0 = env.s\n",
    "   \n",
    "        #\n",
    "        action_t0 = agent.select_action(state_t0)\n",
    "\n",
    "        #\n",
    "        ctr = 0\n",
    "        #pickup_flag = True\n",
    "        while True:\n",
    "\n",
    "            #\n",
    "            #print ('sampled action: ', action_t0)\n",
    "\n",
    "            # take action in enviorment\n",
    "            state_t1, reward_t1, done, action_mask_t1 = env.step(action_t0)\n",
    "            taxi_row, taxi_col, pass_idx, dest_idx = env.decode(env.s)\n",
    "\n",
    "            #\n",
    "            agent.action_mask_t1 = action_mask_t1['action_mask']\n",
    "\n",
    "\n",
    "            # update Q of agent\n",
    "            action_t1 = agent.step_sarsa(state_t0,\n",
    "                                   action_t0,\n",
    "                                   reward_t1,\n",
    "                                   state_t1,\n",
    "                                   #action_t1,\n",
    "                                   done)\n",
    "\n",
    "            #\n",
    "            state_t0 = state_t1\n",
    "            action_t0 = action_t1\n",
    "\n",
    "            #\n",
    "            if done==True:\n",
    "                if verbose:\n",
    "                    if ctr<199:\n",
    "                        print (\"copmleted run \", i_episode, \" ... in # steps: \", ctr)\n",
    "                break\n",
    "\n",
    "            ctr+=1\n",
    "            current_reward+=reward_t1\n",
    "\n",
    "        #\n",
    "        pbar.set_postfix(average_reward=current_reward, refresh=False)\n",
    "        pbar.update(0)\n",
    "        current_reward = 0\n",
    "\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03a411-bf04-47f5-9fa9-dc9c6bff771a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 244/1000 || Best average reward -354.53"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m env\u001b[38;5;241m.\u001b[39mfixed_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m avg_rewards, best_avg_reward \u001b[38;5;241m=\u001b[39m \u001b[43minteract\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mwindow\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#print (\"avg reward: \", avg_rewards)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_avg_reward: \u001b[39m\u001b[38;5;124m\"\u001b[39m, best_avg_reward)\n",
      "File \u001b[0;32m~/code/deep-reinforcement-learning/lab-taxi/monitor.py:40\u001b[0m, in \u001b[0;36minteract\u001b[0;34m(env, agent, num_episodes, window)\u001b[0m\n\u001b[1;32m     37\u001b[0m samp_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# agent selects an action\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# agent performs the selected action\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/code/deep-reinforcement-learning/lab-taxi/agent.py:77\u001b[0m, in \u001b[0;36mAgent.select_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# if all actions are same, then split the probs\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(probs)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 77\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnA), p\u001b[38;5;241m=\u001b[39m\u001b[43mprobs\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnA))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "################### TEST AGENT #######################\n",
    "######################################################\n",
    "#\n",
    "n_episodes = 1000\n",
    "window = 100\n",
    "\n",
    "agent.greedy_policy=True\n",
    "agent.step = agent.step_sarsa\n",
    "\n",
    "# force learning of a single state:\n",
    "env.s = 0\n",
    "env.fixed_state=0\n",
    "\n",
    "#\n",
    "avg_rewards, best_avg_reward = interact(env, \n",
    "                                        agent,\n",
    "                                        n_episodes,\n",
    "                                        window\n",
    "                                       )\n",
    "\n",
    "#print (\"avg reward: \", avg_rewards)\n",
    "print (\"best_avg_reward: \", best_avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e50ff-b797-4c6e-8099-dce612ff7ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3d928-ecbd-47c1-844a-32d78137e502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
